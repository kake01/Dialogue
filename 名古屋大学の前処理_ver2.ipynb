{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "名古屋大学の前処理_ver2",
      "provenance": [],
      "collapsed_sections": [
        "Aokpp2mfol_C",
        "fBUNA8MttoPY",
        "7gdU8aS_o1lK",
        "f6YQ_fnjAREf",
        "V7wDdUi5KI2O",
        "a8zEHClOggou",
        "_Iu70unrg2W1"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kake01/Dialogue/blob/Vectorization/%E5%90%8D%E5%8F%A4%E5%B1%8B%E5%A4%A7%E5%AD%A6%E3%81%AE%E5%89%8D%E5%87%A6%E7%90%86_ver2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J0Qjg6vuaHNt"
      },
      "source": [
        "#名古屋大コーパスの前処理,ベクトル化"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aokpp2mfol_C",
        "colab_type": "text"
      },
      "source": [
        "## 環境構築"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mp_9rFvwpZbs",
        "colab_type": "code",
        "outputId": "23bf940a-38d1-45c4-e696-41b1259fb1ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pip install janome"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: janome in /usr/local/lib/python3.6/dist-packages (0.3.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tnxXKDjq3jEL",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import re\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import pickle\n",
        "from gensim.models import word2vec\n",
        "from janome.tokenizer import Tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wfodePkj3jEa"
      },
      "source": [
        "## データセットの解凍と前処理と保存"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBUNA8MttoPY",
        "colab_type": "text"
      },
      "source": [
        "###データの解凍"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9u9aSW2jrfO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with zipfile.ZipFile('data.zip') as existing_zip:\n",
        "  existing_zip.extractall('data')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gdU8aS_o1lK",
        "colab_type": "text"
      },
      "source": [
        "###データの前処理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kRVATYOgJs1b",
        "colab": {}
      },
      "source": [
        "text = \"\"\n",
        "# s = \"私は柴犬【しばいぬ】とシャム猫【しゃむねこ】が大好きです。\"\n",
        "# s = re.sub(\"【[^】]+】\", \"\", s)  # 【と】の間に】以外の文字が複数ある箇所を、空の文字列に置き換える\n",
        "for data_num in range(129):\n",
        "  # ファイルデータを1つずつ読み込む\n",
        "  with open(\"data/nucc/data_\"+str(data_num+1)+\".txt\", mode=\"r\", encoding=\"utf-8\") as f:  # ファイルの読み込み\n",
        "    texts = f.readline()\n",
        "    while texts:\n",
        "      # 1行ずつ読み込む\n",
        "      texts = re.sub(\"《[^》]+》\", \"\", texts)  # ルビの削除\n",
        "      texts = re.sub(\"［[^］]+］\", \"\", texts)  # 読みの注意の削除\n",
        "      texts = re.sub(\"〔[^〕]+〕\", \"\", texts)  # 読みの注意の削除\n",
        "      texts = re.sub(\"＜[^＞]+＞\", \"\", texts)  # ＜＞の削除\n",
        "      texts = re.sub(\"（[^）]+）\", \"\", texts)  # ()の削除\n",
        "      texts = re.sub(\"＠[^\\n]+\\n\", \"\", texts)  # @から始まる文を削除\n",
        "      # texts = re.sub(\"[ 　\\n「」『』（）｜※＊…]\", \"\", texts)  # 全角半角スペース、改行、その他記号の削除\n",
        "      texts = re.sub(\"[ 　「」『』（）｜※＊…]\", \"\", texts)  # 全角半角スペース、改行、その他記号の削除\n",
        "\n",
        "      if '：' in texts:\n",
        "        # 文頭に名前が書いてある場合のみ名前の削除を行う\n",
        "         texts = re.sub(\"^[^：]+：\", \"\", texts, flags=re.MULTILINE)   #^で行頭を探せる\n",
        "      \n",
        "      text += texts\n",
        "      texts = f.readline()\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6YQ_fnjAREf",
        "colab_type": "text"
      },
      "source": [
        "###データの保存"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SY7WsJQ_3Yt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# コーパスとして保存する\n",
        "with open(\"corpus.txt\", mode=\"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7wDdUi5KI2O",
        "colab_type": "text"
      },
      "source": [
        "###コーパスのサイズ確認"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HA4pMv4iOT76",
        "colab_type": "code",
        "outputId": "27349ae9-4d00-441a-ced4-05245653170c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "with open('corpus.txt') as f:\n",
        "  texts = f.read()\n",
        "print(\"文字数:\", len(texts))\n",
        "# print(texts[:200])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "文字数: 2260139\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NBujIXDFeLp",
        "colab_type": "text"
      },
      "source": [
        "### 文章ごとに単語に分割したリストを保存する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOIiKK3KBFkt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ファイルの読み込み\n",
        "with open(\"corpus.txt\", mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  text = f.read()\n",
        "seperator = \"。\"  # 。をセパレータに指定\n",
        "sentence_list = text.split(seperator)  # セパレーターを使って文章をリストに分割する\n",
        "sentence_list.pop() # 最後の要素は空の文字列になるので、削除\n",
        "sentence_list = [x+seperator for x in sentence_list]  # 文章の最後に。を追加\n",
        "\n",
        "\n",
        "t = Tokenizer()\n",
        "\n",
        "words = []\n",
        "for sentence in sentence_list:\n",
        "    words.append(t.tokenize(sentence, wakati=True))   # 文章ごとに単語に分割し、リストに格納"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2SE33ppW8ml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#保存する\n",
        "with open('words.pickle', mode='wb') as f:  # pickleに保存\n",
        "    pickle.dump(words, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9YH2ClORQVm",
        "colab_type": "code",
        "outputId": "b497db0b-8d01-421c-db94-f4a3a5ab4acd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "print(words[0])\n",
        "print(max([len(x) for x in sentence_list]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['の', '町', 'という', 'の', 'は', 'ちい', 'ちゃ', 'くっ', 'て', '、', '城壁', 'が', 'こう', '町', '全体', 'を', 'ぐるっと', '回っ', 'て', 'て', '、', 'それ', 'が', '城壁', 'の', '上', 'を', '歩い', 'て', 'も', '１', '時間', 'ぐらい', 'です', 'よ', 'ね', '。']\n",
            "371\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAS0B9AH_KYX",
        "colab_type": "text"
      },
      "source": [
        "##単語のベクトル化(word2vec)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJPgzJ0-Gi3V",
        "colab_type": "code",
        "outputId": "4fe5e36f-dc00-4d61-991b-9ca90af1418f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#words.pickleを読み込む\n",
        "with open('words.pickle', 'rb') as web:\n",
        "  words = pickle.load(web)\n",
        "print (words[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['の', '町', 'という', 'の', 'は', 'ちい', 'ちゃ', 'くっ', 'て', '、', '城壁', 'が', 'こう', '町', '全体', 'を', 'ぐるっと', '回っ', 'て', 'て', '、', 'それ', 'が', '城壁', 'の', '上', 'を', '歩い', 'て', 'も', '１', '時間', 'ぐらい', 'です', 'よ', 'ね', '。']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INgSt3S7gZGd",
        "colab_type": "text"
      },
      "source": [
        "###単語のベクトル化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0x1cAJJc_aI2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# size : 中間層のニューロン数 デフォルト100\n",
        "# min_count : この値以下の出現回数の単語を無視\n",
        "# window : 対象単語を中心とした前後の単語数\n",
        "# iter : epochs数\n",
        "# sg : skip-gramを使うかどうか 0:CBOW 1:skip-gram\n",
        "model = word2vec.Word2Vec(words, size=250, min_count=5, window=5, iter=20, sg = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3XnNSbz_rep",
        "colab_type": "code",
        "outputId": "fdfa3a88-28dd-4a89-9411-413cf0a952fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        }
      },
      "source": [
        "print(model.wv.vectors.shape)  # 分散表現の形状\n",
        "print(model.wv.vectors)  # 分散表現"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7452, 250)\n",
            "[[ 2.92988390e-01  1.71209157e-01 -3.46329324e-02 ... -5.99733472e-01\n",
            "  -4.71526161e-02  5.38184822e-01]\n",
            " [-6.37993097e-01 -1.13867566e-01 -4.13113713e-01 ... -1.72156692e-01\n",
            "   4.65985686e-02 -1.82006031e-01]\n",
            " [-2.95263287e-02  4.64924634e-01  5.34544170e-01 ...  6.23028576e-02\n",
            "  -6.42332315e-01 -1.90581053e-01]\n",
            " ...\n",
            " [-1.94737524e-01 -1.27943620e-01  6.15652427e-02 ...  6.99783340e-02\n",
            "  -1.16544012e-02 -1.25720520e-02]\n",
            " [-8.98158103e-02  2.83404690e-04  1.09103560e-01 ... -6.09476957e-03\n",
            "   1.34129245e-02 -5.96308894e-02]\n",
            " [ 3.19119096e-02 -9.51655209e-02 -3.15201283e-02 ... -1.14573007e-02\n",
            "  -5.54407109e-03  4.94537652e-02]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8zEHClOggou",
        "colab_type": "text"
      },
      "source": [
        "###類似語のテスト"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiQQ-c-5_5fs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(model.wv.most_similar(\"怖い\"))  # 最も似ている単語"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Iu70unrg2W1",
        "colab_type": "text"
      },
      "source": [
        "###モデル,行列の保存"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PRZdSiYdFLQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # モデルを保存\n",
        "model.save(\"model.model\")\n",
        "# モデルを読み込む\n",
        "# model = word2vec.Word2Vec.load(\"model.model\")\n",
        "\n",
        "with open('model.pickle', mode='wb') as f:  # pickleに保存\n",
        "    pickle.dump(model.wv.vectors, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieKpTQnjoPhs",
        "colab_type": "text"
      },
      "source": [
        "##辞書の作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQnnRwekoMta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chars = []\n",
        "\n",
        "for char in text:  # コーパスに使われている文字を追加\n",
        "    if char not in chars:\n",
        "        chars += char\n",
        "        \n",
        "chars += \"\\t\\n\"  # タブと改行を追加\n",
        "chars_list = sorted(list(chars))  # 文字列をリストに変換してソートする\n",
        "\n",
        "with open(\"chars.pickle\", mode=\"wb\") as f:  # pickleで保存\n",
        "    pickle.dump(chars_list, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB1zBd7CPTlt",
        "colab_type": "code",
        "outputId": "d9c1ccc5-8be7-4f58-8a97-37d274cbae0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# インデックスと文字で辞書を作成\n",
        "char_indices = {}  # 文字がキーでインデックスが値\n",
        "for i, char in enumerate(chars_list):\n",
        "    char_indices[char] = i\n",
        "indices_char = {}  # インデックスがキーで文字が値\n",
        "for i, char in enumerate(chars_list):\n",
        "    indices_char[i] = char\n",
        "\n",
        "print(len(indices_char))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2378\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYzge22jhIcv",
        "colab_type": "text"
      },
      "source": [
        "##encoder,decoderへの入力,decoderの正解を作成\n",
        "TODO:word2vec使って行う"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiBgjNP7e2Gd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_sentence_length = 128  # 文章の最大長さ。これより長い文章はカットされる。\n",
        "sentence_list = [sentence for sentence in sentence_list if len(sentence) <= max_sentence_length]  # 長すぎる文章のカット\n",
        "\n",
        "n_char = len(chars_list)  # 文字の種類の数\n",
        "n_sample = len(sentence_list) - 1  # サンプル数\n",
        "\n",
        "x_sentences = []  # 入力の文章\n",
        "t_sentences = []  # 正解の文章\n",
        "for i in range(n_sample):\n",
        "    x_sentences.append(sentence_list[i])\n",
        "    t_sentences.append(\"\\t\" + sentence_list[i+1] + \"\\n\")  # 正解は先頭にタブ、末尾に改行を加える\n",
        "max_length_x = max_sentence_length  # 入力文章の最大長さ\n",
        "max_length_t = max_sentence_length + 2  # 正解文章の最大長さ\n",
        "\n",
        "x_encoder = np.zeros((n_sample, max_length_x, n_char), dtype=np.bool)  # encoderへの入力\n",
        "x_decoder = np.zeros((n_sample, max_length_t, n_char), dtype=np.bool)  # decoderへの入力\n",
        "t_decoder = np.zeros((n_sample, max_length_t, n_char), dtype=np.bool)  # decoderの正解\n",
        "\n",
        "for i in range(n_sample):\n",
        "    x_sentence = x_sentences[i]\n",
        "    t_sentence = t_sentences[i]\n",
        "    for j, char in enumerate(x_sentence):\n",
        "        x_encoder[i, j, char_indices[char]] = 1  # encoderへの入力をone-hot表現で表す\n",
        "    for j, char in enumerate(t_sentence):\n",
        "        x_decoder[i, j, char_indices[char]] = 1  # decoderへの入力をone-hot表現で表す\n",
        "        if j > 0:  # 正解は入力より1つ前の時刻のものにする\n",
        "            t_decoder[i, j-1, char_indices[char]] = 1\n",
        "            \n",
        "print(x_encoder.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}