{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "名古屋大学の前処理_ver2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "Aokpp2mfol_C"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kake01/Dialogue/blob/Preprocessing/%E5%90%8D%E5%8F%A4%E5%B1%8B%E5%A4%A7%E5%AD%A6%E3%81%AE%E5%89%8D%E5%87%A6%E7%90%86_ver2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J0Qjg6vuaHNt"
      },
      "source": [
        "#名古屋大コーパスの前処理,ベクトル化"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aokpp2mfol_C",
        "colab_type": "text"
      },
      "source": [
        "## 環境構築"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mp_9rFvwpZbs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pip install janome"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tnxXKDjq3jEL",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import re\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import pickle\n",
        "# from janome.tokenizer import Tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wfodePkj3jEa"
      },
      "source": [
        "## データセットの解凍と前処理と保存"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBUNA8MttoPY",
        "colab_type": "text"
      },
      "source": [
        "###データの解凍"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9u9aSW2jrfO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with zipfile.ZipFile('data.zip') as existing_zip:\n",
        "  existing_zip.extractall('data')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gdU8aS_o1lK",
        "colab_type": "text"
      },
      "source": [
        "###データの前処理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kRVATYOgJs1b",
        "colab": {}
      },
      "source": [
        "text = \"\"\n",
        "# s = \"私は柴犬【しばいぬ】とシャム猫【しゃむねこ】が大好きです。\"\n",
        "# s = re.sub(\"【[^】]+】\", \"\", s)  # 【と】の間に】以外の文字が複数ある箇所を、空の文字列に置き換える\n",
        "for data_num in range(129):\n",
        "  # ファイルデータを1つずつ読み込む\n",
        "  with open(\"data/nucc/data_\"+str(data_num+1)+\".txt\", mode=\"r\", encoding=\"utf-8\") as f:  # ファイルの読み込み\n",
        "    texts = f.readline()\n",
        "    while texts:\n",
        "      # 1行ずつ読み込む\n",
        "      texts = re.sub(\"《[^》]+》\", \"\", texts)  # ルビの削除\n",
        "      texts = re.sub(\"［[^］]+］\", \"\", texts)  # 読みの注意の削除\n",
        "      texts = re.sub(\"〔[^〕]+〕\", \"\", texts)  # 読みの注意の削除\n",
        "      texts = re.sub(\"＜[^＞]+＞\", \"\", texts)  # ＜＞の削除\n",
        "      texts = re.sub(\"（[^）]+）\", \"\", texts)  # ())の削除\n",
        "      texts = re.sub(\"＠[^\\n]+\\n\", \"\", texts)  # @から始まる文を削除\n",
        "      # texts = re.sub(\"[ 　\\n「」『』（）｜※＊…]\", \"\", texts)  # 全角半角スペース、改行、その他記号の削除\n",
        "      texts = re.sub(\"[ 　「」『』（）｜※＊…]\", \"\", texts)  # 全角半角スペース、改行、その他記号の削除\n",
        "\n",
        "      if '：' in texts:\n",
        "        # 文頭に名前が書いてある場合のみ名前の削除を行う\n",
        "         texts = re.sub(\"^[^：]+：\", \"\", texts, flags=re.MULTILINE)   #^で行頭を探せる\n",
        "      \n",
        "      text += texts\n",
        "      texts = f.readline()\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6YQ_fnjAREf",
        "colab_type": "text"
      },
      "source": [
        "###データの保存"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SY7WsJQ_3Yt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# コーパスとして保存する\n",
        "with open(\"corpus.txt\", mode=\"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7wDdUi5KI2O",
        "colab_type": "text"
      },
      "source": [
        "##コーパスのサイズ確認"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HA4pMv4iOT76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('corpus.txt') as f:\n",
        "  texts = f.read()\n",
        "print(\"文字数:\", len(texts))\n",
        "# print(texts[:200])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOIiKK3KBFkt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ファイルの読み込み\n",
        "with open(\"corpus.txt\", mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  text = f.read()\n",
        "seperator = \"。\"  # 。をセパレータに指定\n",
        "sentence_list = text.split(seperator)  # セパレーターを使って文章をリストに分割する\n",
        "sentence_list.pop() # 最後の要素は空の文字列になるので、削除\n",
        "sentence_list = [x+seperator for x in sentence_list]  # 文章の最後に。を追加\n",
        "\n",
        "\n",
        "t = Tokenizer()\n",
        "\n",
        "# words = []\n",
        "# for sentence in sentence_list:\n",
        "#     words.append(t.tokenize(sentence, wakati=True))   # 文章ごとに単語に分割し、リストに格納\n",
        "    \n",
        "# with open('words.pickle', mode='wb') as f:  # pickleに保存\n",
        "#     pickle.dump(words, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAS0B9AH_KYX",
        "colab_type": "text"
      },
      "source": [
        "##単語のベクトル化(word2vec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INgSt3S7gZGd",
        "colab_type": "text"
      },
      "source": [
        "###単語のベクトル化を行う"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0x1cAJJc_aI2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models import word2vec\n",
        "\n",
        "# size : 中間層のニューロン数\n",
        "# min_count : この値以下の出現回数の単語を無視\n",
        "# window : 対象単語を中心とした前後の単語数\n",
        "# iter : epochs数\n",
        "# sg : skip-gramを使うかどうか 0:CBOW 1:skip-gram\n",
        "model = word2vec.Word2Vec(wagahai_words,\n",
        "                          size=100,\n",
        "                          min_count=5,\n",
        "                          window=5,\n",
        "                          iter=20,\n",
        "                          sg = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3XnNSbz_rep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(model.wv.vectors.shape)  # 分散表現の形状\n",
        "print(model.wv.vectors)  # 分散表現"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8zEHClOggou",
        "colab_type": "text"
      },
      "source": [
        "###類似語のテストを行う"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiQQ-c-5_5fs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(model.wv.most_similar(\"怖い\"))  # 最も似ている単語"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Iu70unrg2W1",
        "colab_type": "text"
      },
      "source": [
        "###モデル,行列の保存を行う"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PRZdSiYdFLQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # モデルを保存\n",
        "model.save(\"model.model\")\n",
        "# モデルを読み込む\n",
        "# model = word2vec.Word2Vec.load(\"model.model\")\n",
        "\n",
        "with open('model.pickle', mode='wb') as f:  # pickleに保存\n",
        "    pickle.dump(model.wv.vectors, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieKpTQnjoPhs",
        "colab_type": "text"
      },
      "source": [
        "##辞書の作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQnnRwekoMta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chars = []\n",
        "\n",
        "for char in text:  # コーパスに使われている文字を追加\n",
        "    if char not in chars:\n",
        "        chars += char\n",
        "        \n",
        "chars += \"\\t\\n\"  # タブと改行を追加\n",
        "chars_list = sorted(list(chars))  # 文字列をリストに変換してソートする\n",
        "# print(chars_list)\n",
        "\n",
        "\n",
        "with open(\"temp.pickle\", mode=\"wb\") as f:  # pickleで保存\n",
        "    pickle.dump(chars_list, f)\n",
        "\n",
        "# インデックスと文字で辞書を作成\n",
        "char_indices = {}  # 文字がキーでインデックスが値\n",
        "for i, char in enumerate(chars_list):\n",
        "    char_indices[char] = i\n",
        "indices_char = {}  # インデックスがキーで文字が値\n",
        "for i, char in enumerate(chars_list):\n",
        "    indices_char[i] = char\n",
        "\n",
        "print(indices_char)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYzge22jhIcv",
        "colab_type": "text"
      },
      "source": [
        "##encoder,decoderへの入力,decoderの正解を作成\n",
        "TODO:word2vec使って行う"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiBgjNP7e2Gd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# max_sentence_length = 128  # 文章の最大長さ。これより長い文章はカットされる。\n",
        "# sentence_list = [sentence for sentence in sentence_list if len(sentence) <= max_sentence_length]  # 長すぎる文章のカット\n",
        "\n",
        "# n_char = len(chars_list)  # 文字の種類の数\n",
        "# n_sample = len(sentence_list) - 1  # サンプル数\n",
        "\n",
        "# x_sentences = []  # 入力の文章\n",
        "# t_sentences = []  # 正解の文章\n",
        "# for i in range(n_sample):\n",
        "#     x_sentences.append(sentence_list[i])\n",
        "#     t_sentences.append(\"\\t\" + sentence_list[i+1] + \"\\n\")  # 正解は先頭にタブ、末尾に改行を加える\n",
        "# max_length_x = max_sentence_length  # 入力文章の最大長さ\n",
        "# max_length_t = max_sentence_length + 2  # 正解文章の最大長さ\n",
        "\n",
        "# x_encoder = np.zeros((n_sample, max_length_x, n_char), dtype=np.bool)  # encoderへの入力\n",
        "# x_decoder = np.zeros((n_sample, max_length_t, n_char), dtype=np.bool)  # decoderへの入力\n",
        "# t_decoder = np.zeros((n_sample, max_length_t, n_char), dtype=np.bool)  # decoderの正解\n",
        "\n",
        "# for i in range(n_sample):\n",
        "#     x_sentence = x_sentences[i]\n",
        "#     t_sentence = t_sentences[i]\n",
        "#     for j, char in enumerate(x_sentence):\n",
        "#         x_encoder[i, j, char_indices[char]] = 1  # encoderへの入力をone-hot表現で表す\n",
        "#     for j, char in enumerate(t_sentence):\n",
        "#         x_decoder[i, j, char_indices[char]] = 1  # decoderへの入力をone-hot表現で表す\n",
        "#         if j > 0:  # 正解は入力より1つ前の時刻のものにする\n",
        "#             t_decoder[i, j-1, char_indices[char]] = 1\n",
        "            \n",
        "# print(x_encoder.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}